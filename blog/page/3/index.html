<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv='X-UA-Compatible' content='IE=edge;chrome=1' />
    <title>From Data to Scholarship</title>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/blog/feed.xml" />
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link href="/assets/stylesheets/style.css" media="screen" rel="stylesheet" type="text/css" />
    <link href='http://fonts.googleapis.com/css?family=Coda|Open+Sans:300italic,400italic,400,700' rel='stylesheet' type='text/css'>  
    <link href='http://fonts.googleapis.com/css?family=Forum' rel='stylesheet' type='text/css'>
    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".nav-button").click(function () {
            $(".nav-button,.primary-nav").toggleClass("open");
            });    
        });
    </script>

  </head>  <body>
   <div id="wrap">
      <header id="head">
        <div id="title">
  <h1><a href="/">From Data to Scholarship</a></h1>
  <h2>Digital Experiments in Religious History</h2>
</div>
<nav class="primary-nav">
  <ul>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/portfolio.html">Portfolio</a></li>
    <li><a href="/about.html">About</a></li>
    <li><a href="/cv.html">Vita</a></li>
    <li><a href="/search.html"><i class="fa fa-search"></i></a></li>
  </ul>
</nav>


      </header>
      <main>
        <article>
          <!--<p>Page 3 of 15</p>-->

  
    <p><a href="/blog/page/2/">Previous page</a></p>
  

<section>
  <h2><a href="/blog/2012/11/08/healings-and-holiness-pentacostals-the-black-baptist-church-itinerant-women-and-spiritual-narratives.html">Healings and Holiness, Pentacostals, the Black Baptist Church, Itinerant Women, and Spiritual Narratives</a>, <span> 8 Nov 2012</span></h2>
  <!-- use article.summary(250) if you have Nokogiri available to show just
       the first 250 characters -->
  <p>We are a bit eclectic this week.</p>

<p>Michael Stephens, <em>Who Healeth All Thy Diseases: Health, Healing, and Holiness in the Church of God Reformation Movement</em>.</p>

<p>Stephens examines the history of healing within the Church of God (Anderson) movement, arguing that despite the lack of scholarly attention, healing practices were central to the development of the denomination and of American Protestantism in general. The focus is largely on major figures within the denomination, particularly Daniel Sidney Warner, Enoch E. Byrum, Charles Wesley Naylor, and the Gospel Trumpet Company publications. Stephens traces the changing attitudes toward healing, from viewing healing as the particular sign of sanctification to a repudiation of all medical intervention to a moderate stance, while highlighting the adoption of popular health reform prescriptions as part of the healthy Christian life.</p>

<p>Evelyn Brooks Higginbotham, <em>Righteous Discontent: The Women’s Movement in the Black Baptist Church, 1880-1920.</em></p>

<p>Higginbotham takes up the challenge of writing a history that focuses on women and gender while also arguing for the centrality of women and the construction of gender to the creation of society. She argues that the black church is the “product and process of male and female interaction” and that a focus on “the ministry” misses half of the conversation (2). She deploys the analytical frameworks of Benedict Anderson and Habermas, describing the creation of the church as a “public sphere” and central to “imagined community” of black society. As Higginbotham deftly shows, both the imagined community and the public sphere were contested and shifting and she focuses on the interactions of race, class, and gender to reveal the different discourses in play in shaping the community.</p>

<p>Joycelyn Moody, <em>Sentimental Confessions: Spiritual Narratives of Nineteenth-Century African American Women.</em></p>

<p>*Sentimental Confessions *is a recovery project focused on reclaiming the black women’s spiritual autobiographies as works of theological importance and as rhetorically powerful and significant texts. Moody is interacting with scholarship on sentimentalism and women’s writing during the 19th-century, arguing against the tradition of Ann Douglas and for the idea that sentimental writing falls within the long tradition of religious writing that is embodied, sensual, and emotionally potent. The individual chapters focus more on close readings of the texts themselves and provide little additional historical context. The framing of this text is most interesting for historical projects that work with these and other such primary sources.</p>

<p>Edith Blumhofer, <em>Restoring the Faith: The Assemblies of God, Pentecostalism, and American Culture</em>.</p>

<p><em>Restoring the Faith</em> is an institutional history of the Assemblies of God denomination. Blumhofer traces a historical arch that begins with the restorationist impulse to return the church to the example of the New Testament, that moves through a period of growth and acculturation, and ends at a present day that has become something other than its primitivist beginnings, that has “succumbed to aspects of modernity” that it began in opposition to (12). Her history focuses more on Parham than Seymour, more on the white Pentecostal experience than African-American or Hispanic experiences, and while she does address some questions of women in leadership positions in the Assemblies of God denomination, how gender is constructed and negotiated is not the focus of her study. The book provides a useful overview and reference for some of the major moments and figures in the institutional history of the Assemblies of God.</p>

<p>Elizabeth Elkin Grammer, <em>Some Wild Visions: Autobiographies of Female Itinerant Evangelists in Nineteenth-Century America.</em></p>

<p>Grammer’s text is a literary study of the autobiographies of 7 female itinerant preachers: Nancy Towle, Jarena Lee, Zilpha Elaw, Lydia Sexton, Laura Haviland, Julia Foote, and Amanda Smith. In analyzing their self-presentation, Grammer focuses on 4 shared characteristics these autobiographies share and seeks to interpret the role and experience of these female itinerants on the basis of those shared characteristics. She notes a shared conscious leaving of the domestic space and the ideal of domesticity, and even an intentional “mis-reading” of that ideal so as to allow them to claim the world as their domestic space (56). She notes that they take part in the more “masculine” culture of “competitive individualism and accounting,” listing for readers the preaching engagements held and the number of converts attained (70). She notes that they were largely alone and also saw themselves as separate in some way from the general culture. And finally she comments upon a shared style in autobiographical writing and the lack of clear ordering or narrative that marks all of these texts. Her work studies how these women combined competing paradigms of “domesticity … individual assertion and competitiveness… and reluctant prophet and suffering savior” to “inhabit and subvert many assumptions about gender, race, and class” (23).</p>

<hr />

<p>More questions and thoughts than synthesis this week.</p>

<p>One of the interesting methodological questions I have coming out of these readings is the question of dealing with women and women’s organizations as a group and focusing on individual women within religious movements. Moody and Higginbotham take two different approaches to this – Moody, because of her focus on the autobiography, looks at the rhetoric of 6 individual women writers while Higginbotham looks at women’s organizations within the Black Baptist church and how those organizations influenced the overall institution. While I see Higginbotham’s point that only studying individual women often leaves them still outside of the larger narrative about society and culture, how does one deal responsibly (avoid generalizations, etc) when telling the history of a group.</p>

<p>Grammer notes that “revival” is a dominant theme throughout the 19th century. However, in reading her and other texts, it seems that the millennium is perhaps the stronger theme, driving even revival itself. Revival for the sake of revival does not seem strong enough to move so many people in so many extraordinary directions. However, and this is pulling from Abzug as well, the firm belief in the coming millennium seems like it could be enough to provoke the slew of revivals and reform efforts that we see.</p>

<p>Both Moody and Grammer are interested in using these text, which on the surface seem to restate rather than challenge cultural ideologies, to uncover how individual women negotiated and challenged those ideologies. Grammer’s text in general offers some very interesting and instructive ways of readings these texts – I need to return to this text for a closer reading. And Moody quotes Sharon Harris to argue that even texts that do perpetuate dominant ideology are worthy of study, as they are sites where ideologies are interacted with, help us understand the conditions under which the text was produced and also often contain subversions of those ideologies (181). Both of these are useful resources for thinking about how to deal with these texts going further.</p>

<p>While neither Stephens nor Blumhofer ignore women in their histories, it is clear that the focus of investigation is the institution and the (generally) male leaders who lead and influenced those institutions. On the one hand, this doesn’t seem unreasonable – the institution and its records is largely controlled by the male leadership. However, Higginbotham seems to offer a solid critique of this approach by arguing that it is the interactions between male and female individuals and groups that create the larger culture and institution. Can Higginbotham’s approach be applied further? Can it be argued that one cannot understand any religious group apart from the ongoing conversations and negotiations between female auxiliary organizations and the traditionally male controlled hierarchy (in whatever form that takes)?</p>

</section>
<section>
  <h2><a href="/blog/2012/11/04/slides-from-text-mining-presentation.html">Slides from Text-Mining Presentation</a>, <span> 4 Nov 2012</span></h2>
  <!-- use article.summary(250) if you have Nokogiri available to show just
       the first 250 characters -->
  <p>Bit short since I was trying to read off of the screen less. Down-side: slides are less helpful on their own.</p>

<p>The code examples are available at <a href="https://github.com/jerielizabeth/mining-example">github.com/jerielizabeth/mining-example</a></p>


</section>
<section>
  <h2><a href="/blog/2012/11/04/short-tutorial-cleaning-up-downloaded-files-draft.html">Short Tutorial: Cleaning up downloaded files (draft)</a>, <span> 4 Nov 2012</span></h2>
  <!-- use article.summary(250) if you have Nokogiri available to show just
       the first 250 characters -->
  <p>So you used wget or Python to pull down a collection of files from the web. Excellent! But in looking through your loot, you notice that a number of the files are oddly small and, on further examination, find that they are functionally empty. How do you clean up your collection of files quickly?</p>

<p>There are a number of very powerful command line tools built into UNIX systems (Mac and Linux) that allow you to manipulate your files quickly and easily. This is a brief tutorial on how to use those tools to locate all of the files that are too small and then remove those files from your collection.</p>

<p>Begin by navigating in Terminal to your collection of folders. For example, my files were located a couple of folders downs within my Documents folder.</p>

<pre>cd Documents/Github/Clio3/Webscraping/hymn-files</pre>

<p>Once in the folder with your downloaded files, you need to find a way to isolate out the files that are too small to be interesting. To do this, use the “find” command. First, to see the various options associated with “find”, enter</p>

<pre>man help</pre>

<p>Use the up and down arrows to move around the window that appears. Look around at the various options – there are many! However, because we are looking only in a particular folder, which in my case has no subfolders, all that we need to look at for now is size.</p>

<p>To exit this window, type “q”.</p>

<p>If the files we are interested in sorting through are all one file type (in my case they’re .json files), we can tell the computer to find all of the files of a particular type and particular size as follows:</p>

<pre>find *.json -size 28c</pre>

<p>This would find all of the json files that are 28 bytes in size.</p>

<p>However, we want all the files that are 28 bytes or less.</p>

<pre>find *.json -size -28c</pre>

<p>Finding the files is great, but now we need to do something with that collection of files.</p>

<p>First we are going to “pipe” the results of our search over to our second, removal, function.</p>

<p>(From here on the examples are showing how the command is built – Do not run the command until the very end when all the pieces are in place)</p>

<pre>find *.json -size -28c |</pre>

<p>Next we are going to use xargs, which helps the computer handle a long list of file names.</p>

<p>Note: The wikipedia entry on xargs suggests using “-0″ (zero) when dealing with file names with spaces in them as xargs defaults to separating at white space (another reason to avoid spaces in filenames). If you run this command without -0 and it doesn’t work, try adding the -0.</p>

<pre>find *.json -size -28c | xargs</pre>

<p>And the function we want to run on each filename is “rm” or remove. rm has a number of options that you can research, some of which really make data un-recoverable and should be used with care. However, a basic “rm” command will be sufficient for this example.</p>

<pre>find *.json -size -28c | xargs rm</pre>

<p>Run this command to remove all of the files less than 28 bytes from your current directory.</p>

<p>(I used 28 bytes because the computer was having trouble with 0 and all of the files I wanted to keep were larger than 28 bytes. Not exactly sure why 0 was a problem but this is why experimenting with the find options before moving on to removing the files is a good idea!)</p>

</section>
<section>
  <h2><a href="/blog/2012/11/04/beautiful-soup-tutorial.html">Beautiful Soup Tutorial</a>, <span> 4 Nov 2012</span></h2>
  <!-- use article.summary(250) if you have Nokogiri available to show just
       the first 250 characters -->
  <p><em>(This tutorial is cross posted at <a href="http://programminghistorian.org/lessons/intro-to-beautiful-soup">The Programming Historian</a>)</em> </p>

<p>Version: Python 2.7.2 and BeautifulSoup 4.</p>

<p>This tutorial assumes basic knowledge of HTML, CSS, and the Document Object Model. It also assumes some knowledge of Python. For a more basic introduction to Python, see <a href="http://programminghistorian.org/lessons/working-with-text-files">Working with Text Files</a>. Most of the work is done in the terminal. For an introduction to using the terminal, see the Scholar’s Lab <a href="http://praxis.scholarslab.org/tutorials/bash/">Command Line Bootcamp</a> tutorial.</p>

<h2 id="what-is-beautiful-soup">What is Beautiful Soup?</h2>

<h3 id="overview">Overview</h3>

<p>“You didn’t write that awful page. You’re just trying to get some data out of it. Beautiful Soup is here to help.” (<a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/">Opening lines of Beautiful Soup</a>) Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages. Say you’ve found some webpages that display data relevant to your research, such as date or address information, but that do not provide any way of downloading the data directly. Beautiful Soup helps you pull particular content from a webpage, remove the HTML markup, and save the information. It is a tool for web scraping that helps you clean up and parse the documents you have pulled down from the web. The <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup documentation</a> will give you a sense of variety of things that the Beautiful Soup library will help with, from isolating titles and links, to extracting all of the text from the html tags, to altering the HTML within the document you’re working with.</p>

<h3 id="installing-beautiful-soup">Installing Beautiful Soup</h3>

<p>Installing Beautiful Soup is easiest if you have pip or another Python installer already in place. If you don’t have pip, run through a quick tutorial on <a href="http://programminghistorian.org/lessons/installing-pip-and-beautiful-soup">installing both pip and Beautiful Soup</a> to get it running. Once you have pip installed, run the following command in the terminal to install Beautiful Soup:</p>

<pre class="brush: plain; title: ; notranslate" title="">pip install beautifulsoup4</pre>

<p>You may need to preface this line with “sudo”, which gives your computer permission to write to your root directories and requires you to re-enter your password. This is the same logic behind you being prompted to enter your password when you install a new program. With sudo, the command is:</p>

<pre class="brush: plain; title: ; notranslate" title="">sudo pip install beautifulsoup4</pre>

<div class="wp-caption aligncenter" style="width: 370px">
  <a href="http://xkcd.com/149/"><img alt="" src="http://imgs.xkcd.com/comics/sandwich.png" width="360" height="299" /></a><p class="wp-caption-text">
    The power of sudo<br />&#8220;Sandwich&#8221; by XKCD
  </p>
</div>

<h2 id="application-extracting-names-and-urls-from-an-html-page">Application: Extracting names and URLs from an HTML page</h2>

<h3 id="preview-where-we-are-going">Preview: Where we are going</h3>

<p>Because I like to see where the finish line is before starting, I will begin with a view of what we are trying to create. We are attempting to go from a search results page where the html page looks like this:</p>

<pre class="brush: plain; title: ; notranslate" title="">&lt;/pre&gt;
&lt;table border="1" cellspacing="2" cellpadding="3"&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;Member Name&lt;/th&gt;
&lt;th&gt;Birth-Death&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035"&gt;ADAMS, George Madison&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1837-1920&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074"&gt;ALBERT, William Julian&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1816-1879&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077"&gt;ALBRIGHT, Charles&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1830-1880&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;</pre>

<p>to a CSV file with names and urls that looks like this:</p>

<pre>"ADAMS, George Madison",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035
"ALBERT, William Julian",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074
"ALBRIGHT, Charles",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077</pre>

<p>using a Python script like this:</p>

<pre class="brush: python; title: ; notranslate" title="">from bs4 import BeautifulSoup
import csv
 
soup = BeautifulSoup (open("43rd-congress.html"))
 
final_link = soup.p.a
final_link.decompose()
 
f = csv.writer(open("43rd_Congress.csv", "w"))
f.writerow(["Name", "Link"])    # Write column headers as the first line
 
links = soup.find_all('a')
for link in links:
    names = link.contents[0]
    fullLink = link.get('href')
 
    f.writerow([names,fullLink])
</pre>

<p>This tutorial explains to how to assemble the final code.</p>

<h3 id="get-a-webpage-to-scrape">Get a webpage to scrape</h3>

<p>The first step is getting a copy of the HTML page(s) want to scrape. You can combine BeautifulSoup with <a href="http://urllib3.readthedocs.org/en/latest/">urllib3</a> to work directly with pages on the web. This tutorial, however, focuses on using BeautifulSoup with local (downloaded) copies of html files. The Congressional database that we’re using is not an easy one to scrape because the URL for the search results remains the same regardless of what you’re searching for. While this can be bypassed programmatically, it is easier for our purposes to go to <a href="http://bioguide.congress.gov/biosearch/biosearch.asp" target="_blank">http://bioguide.congress.gov/biosearch/biosearch.asp</a>, search for Congress number 43, and to save a copy of the results page.</p>

<div id="attachment_2108" class="wp-caption aligncenter" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Congressional-Biographical-Directory-CLERKWEB-2013-08-23-12-22-12.jpg"><img class="size-medium wp-image-2108 " alt="Figure 1: BioGuide Interface Search for 43rd Congress " src="http://programminghistorian.org/wp-content/uploads/2012/12/Congressional-Biographical-Directory-CLERKWEB-2013-08-23-12-22-12-300x258.jpg" width="300" height="258" /></a><p class="wp-caption-text">
    Figure 1: BioGuide Interface<br />Search for 43rd Congress
  </p>
</div>

<div id="attachment_2109" class="wp-caption aligncenter" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Congressional-Biographical-Directory-Results-2013-08-23-12-25-09.jpg"><img class="size-medium wp-image-2109 " alt="Figure 2: BioGuide Results We want to download the HTML behind this page." src="http://programminghistorian.org/wp-content/uploads/2012/12/Congressional-Biographical-Directory-Results-2013-08-23-12-25-09-300x234.jpg" width="300" height="234" /></a><p class="wp-caption-text">
    Figure 2: BioGuide Results<br />We want to download the HTML behind this page
  </p>
</div>

<p>Selecting “File” and “Save Page As …” from your browser window will accomplish this (life will be easier if you avoid using spaces in your filename). I have used “43rd-congress.html”. Move the file into the folder you want to work in. (To learn how to automate the downloading of HTML pages using Python, see <a href="http://programminghistorian.org/lessons/automated-downloading-with-wget">Automated Downloading with Wget</a> and <a href="http://programminghistorian.org/lessons/downloading-multiple-records-using-query-strings">Downloading Multiple Records Using Query Strings</a>.)</p>

<h3 id="identify-content">Identify content</h3>

<p>One of the first things Beautiful Soup can help us with is locating content that is buried within the HTML structure. Beautiful Soup allows you to select content based upon tags (example: soup.body.p.b finds the first bold item inside a paragraph tag inside the body tag in the document). To get a good view of how the tags are nested in the document, we can use the method “prettify” on our soup object. Create a new text file called “soupexample.py” in the same location as your downloaded HTML file. This file will contain the Python script that we will be developing over the course of the tutorial. To begin, import the Beautiful Soup library, open the HTML file and pass it to Beautiful Soup, and then print the “pretty” version in the terminal.</p>

<pre class="brush: python; title: ; notranslate" title="">from bs4 import BeautifulSoup soup = BeautifulSoup(open("43rd-congress.html")) print(soup.prettify()) </pre>

<p>Save “soupexample.py” in the folder with your HTML file and go to the command line. Navigate (use ‘cd’) to the folder you’re working in and execute the following:</p>

<pre class="brush: plain; title: ; notranslate" title="">python soupexample.py</pre>

<p>You should see your terminal window fill up with a nicely indented version of the original html text (see Figure 3). This is a visual representation of how the various tags relate to one another.</p>

<div id="attachment_2110" class="wp-caption aligncenter" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-103×40-2013-08-23-13-13-01.jpg"><img class="size-medium wp-image-2110 " alt="Figure 3: &quot;Pretty&quot; print of the BioGuide results" src="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-103×40-2013-08-23-13-13-01-300x242.jpg" width="300" height="242" /></a><p class="wp-caption-text">
    Figure 3: &#8220;Pretty&#8221; print of the BioGuide results
  </p>
</div>

<h3 id="using-beautifulsoup-to-select-particular-content">Using BeautifulSoup to select particular content</h3>

<p>Remember that we are interested in only the names and URLs of the various member of the 43rd Congress. Looking at the ”pretty” version of the file, the first thing to notice is that the data we want is not too deeply embedded in the HTML structure. Both the names and the URLs are, most fortunately, embedded in “<a>” tags. So, we need to isolate out all of the “<a>” tags. We can do this by updating the code in “soupexample.py” to the following:</a></a></p>

<pre class="brush: python; highlight: [5,7,8]; title: ; notranslate" title="">from bs4 import BeautifulSoup

soup = BeautifulSoup (open("43rd-congress.html"))

links = soup.find_all('a')

for link in links:
    print link
</pre>

<p>Save and run the script again to see all of the anchor tags in the document.</p>

<pre class="brush: plain; title: ; notranslate" title="">python soupexample.py</pre>

<p>One thing to notice is that there is an additional link in our file – the link for an additional search.</p>

<div id="attachment_2112" class="wp-caption alignnone" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-101×26-2013-08-23-13-25-56.jpg"><img class="size-medium wp-image-2112" alt="Figure 4: The URLs and names, plus one addition." src="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-101×26-2013-08-23-13-25-56-300x164.jpg" width="300" height="164" /></a><p class="wp-caption-text">
    Figure 4: The URLs and names, plus one addition
  </p>
</div>

<p>We can get rid of this with just a few lines of code. Going back to the pretty version, notice that this last “<a>” tag is not within the table but is within a “&lt;p&gt;” tag.</a></p>

<div id="attachment_2111" class="wp-caption alignnone" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-103×40-2013-08-23-13-23-07.jpg"><img class="size-medium wp-image-2111" alt="Figure 4: The rogue link" src="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-103×40-2013-08-23-13-23-07-300x242.jpg" width="300" height="242" /></a><p class="wp-caption-text">
    Figure 5: The rogue link
  </p>
</div>

<p>Because Beautiful Soup allows us to modify the HTML, we can remove the “<a>” that is under the “&lt;p&gt;” before searching for all the “<a>” tags. To do this, we can use the “decompose” method, which removes the specified content from the “soup”. Do be careful when using “decompose”—you are deleting both the HTML tag and all of the data inside of that tag. If you have not correctly isolated the data, you may be deleting information that you wanted to extract. Update the file as below and run again.</a></a></p>

<pre class="brush: python; highlight: [5,6]; title: ; notranslate" title="">from bs4 import BeautifulSoup

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

links = soup.find_all('a')

for link in links:
    print link
</pre>

<div id="attachment_2113" class="wp-caption alignnone" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-101×26-2013-08-23-13-28-04.jpg"><img class="size-medium wp-image-2113" alt="Figure 6: Successfully isolated only names and URLs" src="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-101×26-2013-08-23-13-28-04-300x164.jpg" width="300" height="164" /></a><p class="wp-caption-text">
    Figure 6: Successfully isolated only names and URLs
  </p>
</div>

<p>Success! We have isolated out all of the links we want and none of the links we don’t!</p>

<h3 id="stripping-tags-and-writing-content-to-a-csv-file">Stripping Tags and Writing Content to a CSV file</h3>

<p>But, we are not done yet! There are still HTML tags surrounding the URL data that we want. And we need to save the data into a file in order to use it for other projects. In order to clean up the HTML tags and split the URLs from the names, we need to isolate the information from the anchor tags. To do this, we will use two powerful, and commonly used Beautiful Soup methods: contents and get. Where before we told the computer to print each link, we now want the computer to separate the link into its parts and print those separately. For the names, we can use link.contents. The “contents” method isolates out the text from within html tags. For example, if you started with</p>

<pre>&lt;h2&gt;This is my Header text&lt;/h2&gt;</pre>

<p>you would be left with “This is my Header text” after applying the contents method. In this case, we want the contents inside the first tag in “link”. (There is only one tag in “link”, but since the computer doesn’t realize that, we must tell it to use the first tag.) For the URL, however, “contents” does not work because the URL is part of the HTML tag. Instead, we will use “get”, which allow us to pull the text associated with (is on the other side of the “=” of) the “href” element.</p>

<pre class="brush: python; highlight: [10,11,12,13]; title: ; notranslate" title="">from bs4 import BeautifulSoup

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

links = soup.find_all('a')
for link in links:
    names = link.contents[0]
    fullLink = link.get('href')
    print names
    print fullLink
</pre>

<div id="attachment_2114" class="wp-caption alignnone" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-101×26-2013-08-23-14-13-13.jpg"><img class="size-medium wp-image-2114" alt="Figure 7: All HTML tags have been removed" src="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-101×26-2013-08-23-14-13-13-300x164.jpg" width="300" height="164" /></a><p class="wp-caption-text">
    Figure 7: All HTML tags have been removed
  </p>
</div>

<p>Finally, we want to use the CSV library to write the file. First, we need to import the CSV library into the script with “import csv.” Next, we create the new CSV file when we “open” it using “csv.writer”. The “w” tells the computer to “write” to the file. And to keep everything organized, let’s write some column headers. Finally, as each line is processed, the name and URL information is written to our CSV file.</p>

<pre class="brush: python; highlight: [2,9,10,14,15,17]; title: ; notranslate" title="">from bs4 import BeautifulSoup
import csv

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

f = csv.writer(open("43rd_Congress.csv", "w"))
f.writerow(["Name", "Link"]) # Write column headers as the first line

links = soup.find_all('a')
for link in links:
    names = link.contents[0]
    fullLink = link.get('href')

    f.writerow([names, fullLink])
</pre>

<p>When executed, this gives us a clean CSV file that we can then use for other purposes.</p>

<div id="attachment_2115" class="wp-caption alignnone" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/43rd_Congress-2-2013-08-23-14-18-27.jpg"><img class="size-medium wp-image-2115" alt="Figure 8: CSV file of results" src="http://programminghistorian.org/wp-content/uploads/2012/12/43rd_Congress-2-2013-08-23-14-18-27-300x125.jpg" width="300" height="125" /></a><p class="wp-caption-text">
    Figure 8: CSV file of results
  </p>
</div>

<p>We have solved our puzzle and have extracted names and URLs from the HTML file.</p>

<hr />

<h2 id="but-wait-what-if-i-want-all-of-the-data">But wait! What if I want ALL of the data?</h2>

<p>Let’s extend our project to capture all of the data from the webpage. We know all of our data can be found inside a table, so let’s use “&lt;tr&gt;” to isolate the content that we want.</p>

<pre class="brush: python; highlight: [8,9,10]; title: ; notranslate" title="">from bs4 import BeautifulSoup

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

trs = soup.find_all('tr')
for tr in trs:
    print tr
</pre>

<p>Looking at the print out in the terminal, you can see we have selected a lot more content than when we searched for “<a>” tags. Now we need to sort through all of these lines to separate out the different types of data.</a></p>

<div id="attachment_2117" class="wp-caption alignnone" style="width: 310px">
  <a href="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-142×40-2013-08-23-16-51-22.jpg"><img class="size-medium wp-image-2117" alt="Figure 8: All of the Table Row data" src="http://programminghistorian.org/wp-content/uploads/2012/12/Beautiful-Soup-Tutorial-—-bash-—-142×40-2013-08-23-16-51-22-300x176.jpg" width="300" height="176" /></a><p class="wp-caption-text">
    Figure 8: All of the Table Row data
  </p>
</div>

<h3 id="extracting-the-data">Extracting the Data</h3>

<p>We can extract the data in two moves. First, we will isolate the link information; then, we will parse the rest of the table row data. For the first, let’s create a loop to search for all of the anchor tags and “get” the data associated with “href”.</p>

<pre class="brush: python; highlight: [12,13,14]; title: ; notranslate" title="">from bs4 import BeautifulSoup

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

trs = soup.find_all('tr')

for tr in trs:
    for link in tr.find_all('a'):
        fulllink = link.get ('href')
        print fulllink #print in terminal to verify results
</pre>

<p>We then need to run a search for the table data within the table rows. (The “print” here allows us to verify that the code is working but is not necessary.)</p>

<pre class="brush: python; highlight: [15,16]; title: ; notranslate" title="">from bs4 import BeautifulSoup

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

trs = soup.find_all('tr')

for tr in trs:
    for link in tr.find_all('a'):
        fulllink = link.get ('href')
        print fulllink #print in terminal to verify results

    tds = tr.find_all("td")
    print tds
</pre>

<p>Next, we need to extract the data we want. We know that everything we want for our CSV file lives within table data (“td”) tags. We also know that these items appear in the same order within the row. Because we are dealing with lists, we can identify information by its position within the list. This means that the first data item in the row is identified by [0], the second by <a href="http://programminghistorian.org/lessons/intro-to-beautiful-soup">1</a>, etc. Because not all of the rows contain the same number of data items, we need to build in a way to tell the script to move on if it encounters an error. This is the logic of the “try” and “except” block. If a particular line fails, the script will continue on to the next line.</p>

<pre class="brush: python; highlight: [17,18,19,20,21,22,23,24,25,26,27,28,29]; title: ; notranslate" title="">from bs4 import BeautifulSoup

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

trs = soup.find_all('tr')

for tr in trs:
    for link in tr.find_all('a'):
        fulllink = link.get ('href')
        print fulllink #print in terminal to verify results

    tds = tr.find_all("td")

    try: #we are using "try" because the table is not well formatted. This allows the program to continue after encountering an error.
        names = str(tds[0].get_text()) # This structure isolate the item by its column in the table and converts it into a string.
        years = str(tds[1].get_text())
        positions = str(tds[2].get_text())
        parties = str(tds[3].get_text())
        states = str(tds[4].get_text())
        congress = tds[5].get_text()

    except:
        print "bad tr string"
        continue #This tells the computer to move on to the next item after it encounters an error

    print names, years, positions, parties, states, congress
</pre>

<p>Within this we are using the following structure:</p>

<pre class="brush: python; title: ; notranslate" title="">years = str(tds[1].get_text())</pre>

<p>We are applying the “get_text” method to the 2nd element in the row (because computers count beginning with 0) and creating a string from the result. This we assign to the variable “years”, which we will use to create the CSV file. We repeat this for every item in the table that we want to capture in our file.</p>

<h3 id="writing-the-csv-file">Writing the CSV file</h3>

<p>The last step in this file is to create the CSV file. Here we are using the same process as we did in Part I, just with more variables. As a result, our file will look like:</p>

<pre class="brush: python; highlight: [2,9,10,32]; title: ; notranslate" title="">from bs4 import BeautifulSoup
import csv

soup = BeautifulSoup (open("43rd-congress.html"))

final_link = soup.p.a
final_link.decompose()

f= csv.writer(open("43rd_Congress_all.csv", "w"))   # Open the output file for writing before the loop
f.writerow(["Name", "Years", "Position", "Party", "State", "Congress", "Link"]) # Write column headers as the first line

trs = soup.find_all('tr')

for tr in trs:
    for link in tr.find_all('a'):
        fullLink = link.get ('href')

    tds = tr.find_all("td")

    try: #we are using "try" because the table is not well formatted. This allows the program to continue after encountering an error.
        names = str(tds[0].get_text()) # This structure isolate the item by its column in the table and converts it into a string.
        years = str(tds[1].get_text())
        positions = str(tds[2].get_text())
        parties = str(tds[3].get_text())
        states = str(tds[4].get_text())
        congress = tds[5].get_text()

    except:
        print "bad tr string"
        continue #This tells the computer to move on to the next item after it encounters an error

    f.writerow([names, years, positions, parties, states, congress, fullLink])
</pre>

<p>You’ve done it! You have created a CSV file from all of the data in the table, creating useful data from the confusion of the html page.</p>


</section>
<section>
  <h2><a href="/blog/2012/11/01/question-for-this-weeks-digital-praxis.html">Question for this week&#8217;s Digital Praxis</a>, <span> 1 Nov 2012</span></h2>
  <!-- use article.summary(250) if you have Nokogiri available to show just
       the first 250 characters -->
  <p>So I’ve been working on text-mining all week and that has generally gone very well.</p>

<p>I am a little less confident on the libraries used to create visualizations from text-mining results using python. The main one I have seen referenced is <a href="http://matplotlib.org/1.1.1/">matplotlab</a>, which is slightly intimidating and feels more science/math focused.</p>

<p>Are there python libraries that humanities types use for visualizations? Are other languages/programs preferred? How do you create the graphical representation of your humanities data?</p>

<p>(I may be jumping ahead a bit here, but I have this list of word frequencies and it feels sad to leave it as a list.)</p>


</section>

  
  <div id="next">
    <p><a href="/blog/page/4/">Next page</a></p>
  </div>
  

        </article>
        <aside>
          <section>
  <h2>Topics</h2>
  <ul>
      <li class="important"><a href="/research.html">Religious History</a></li>
      <li class="important"><a href="/digital.html">Digital History</a></li>
      <li class="important"><a href="/teaching.html">Teaching</a></li>
  </ul>
</section>
<section>
  <h2>Recent Posts</h2>
  <ul>
  
    <li><a href="/blog/2014/07/08/Summer-of-Research-Part-1.html">Summer of Research, Part I</a>, <span>Jul  8</span></li>
  
    <li><a href="/blog/2014/06/12/lessons-from-dhsi.html">All Models are Wrong</a>, <span>Jun 12</span></li>
  
    <li><a href="/blog/2014/03/12/Moving-to-Middleman.html">Moving to Middleman</a>, <span>Mar 12</span></li>
  
  </ul>
</section>
<section>
  <h2>Categories</h2>
  <ul>
    
      <li><a href="/blog/category/research.html">research (27)</a></li>
    
      <li><a href="/blog/category/digital.html">digital (47)</a></li>
    
      <li><a href="/blog/category/coursework.html">coursework (61)</a></li>
    
      <li><a href="/blog/category/teaching.html">teaching (3)</a></li>
    
  </ul>
</section>
        </aside>
      </main>
      <footer>
        <div id="copyright">
  <p>CC-BY Jeri Wieringa, 2014
</div>
<div id="social" class="nav">
  <ul>
    <li><a href="http://twitter.com/jeriwieringa"><i class="fa fa-twitter"></i></a></li>
    <li><a href="https://github.com/jerielizabeth"><i class="fa fa-github"></i></a></li>
    <li><a href="/blog/feed.xml"><i class="fa fa-rss"></i></a></li>
  </ul>
</div>

<!--<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35130770-1', 'jeriwieringa.com');
  ga('send', 'pageview');

</script> -->
      </footer>
    </div>
  </body>
</html>
